{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef657047-48fd-4716-9476-7ea41b8b73f2",
   "metadata": {},
   "source": [
    "The first part is to convert dat file from Lidar prototype to csv file (This can be done by running the first cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77f6db26-67a0-4016-94ae-50674987cdcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processing 05-01-2026 ===\n",
      "[OK] 05-01-2026 00.05: 2000 rows → csvfiles/05-01-2026/00.05.csv\n",
      "[OK] 05-01-2026 00.35: 2000 rows → csvfiles/05-01-2026/00.35.csv\n",
      "[OK] 05-01-2026 01.05: 2000 rows → csvfiles/05-01-2026/01.05.csv\n",
      "[OK] 05-01-2026 01.35: 2000 rows → csvfiles/05-01-2026/01.35.csv\n",
      "[OK] 05-01-2026 02.05: 2000 rows → csvfiles/05-01-2026/02.05.csv\n",
      "[OK] 05-01-2026 02.35: 2000 rows → csvfiles/05-01-2026/02.35.csv\n",
      "[OK] 05-01-2026 03.05: 2000 rows → csvfiles/05-01-2026/03.05.csv\n",
      "[OK] 05-01-2026 03.35: 2000 rows → csvfiles/05-01-2026/03.35.csv\n",
      "[OK] 05-01-2026 04.05: 2000 rows → csvfiles/05-01-2026/04.05.csv\n",
      "[OK] 05-01-2026 04.35: 2000 rows → csvfiles/05-01-2026/04.35.csv\n",
      "[OK] 05-01-2026 05.05: 2000 rows → csvfiles/05-01-2026/05.05.csv\n",
      "[OK] 05-01-2026 05.35: 2000 rows → csvfiles/05-01-2026/05.35.csv\n",
      "[OK] 05-01-2026 06.05: 2000 rows → csvfiles/05-01-2026/06.05.csv\n",
      "[OK] 05-01-2026 06.35: 2000 rows → csvfiles/05-01-2026/06.35.csv\n",
      "[OK] 05-01-2026 07.05: 2000 rows → csvfiles/05-01-2026/07.05.csv\n",
      "[OK] 05-01-2026 07.35: 2000 rows → csvfiles/05-01-2026/07.35.csv\n",
      "[OK] 05-01-2026 08.05: 2000 rows → csvfiles/05-01-2026/08.05.csv\n",
      "[OK] 05-01-2026 08.35: 2000 rows → csvfiles/05-01-2026/08.35.csv\n",
      "[OK] 05-01-2026 09.05: 2000 rows → csvfiles/05-01-2026/09.05.csv\n",
      "[OK] 05-01-2026 09.35: 2000 rows → csvfiles/05-01-2026/09.35.csv\n",
      "[OK] 05-01-2026 10.05: 2000 rows → csvfiles/05-01-2026/10.05.csv\n",
      "[OK] 05-01-2026 10.35: 2000 rows → csvfiles/05-01-2026/10.35.csv\n",
      "[OK] 05-01-2026 11.05: 2000 rows → csvfiles/05-01-2026/11.05.csv\n",
      "[OK] 05-01-2026 11.35: 2000 rows → csvfiles/05-01-2026/11.35.csv\n",
      "[OK] 05-01-2026 12.05: 2000 rows → csvfiles/05-01-2026/12.05.csv\n",
      "[OK] 05-01-2026 12.35: 2000 rows → csvfiles/05-01-2026/12.35.csv\n",
      "[OK] 05-01-2026 13.05: 2000 rows → csvfiles/05-01-2026/13.05.csv\n",
      "[OK] 05-01-2026 13.35: 2000 rows → csvfiles/05-01-2026/13.35.csv\n",
      "[OK] 05-01-2026 14.05: 2000 rows → csvfiles/05-01-2026/14.05.csv\n",
      "[OK] 05-01-2026 14.35: 2000 rows → csvfiles/05-01-2026/14.35.csv\n",
      "[OK] 05-01-2026 15.05: 2000 rows → csvfiles/05-01-2026/15.05.csv\n",
      "[OK] 05-01-2026 15.35: 2000 rows → csvfiles/05-01-2026/15.35.csv\n",
      "[OK] 05-01-2026 16.05: 2000 rows → csvfiles/05-01-2026/16.05.csv\n",
      "[OK] 05-01-2026 16.35: 2000 rows → csvfiles/05-01-2026/16.35.csv\n",
      "[OK] 05-01-2026 17.05: 2000 rows → csvfiles/05-01-2026/17.05.csv\n",
      "[OK] 05-01-2026 17.35: 2000 rows → csvfiles/05-01-2026/17.35.csv\n",
      "[OK] 05-01-2026 18.05: 2000 rows → csvfiles/05-01-2026/18.05.csv\n",
      "[OK] 05-01-2026 18.35: 2000 rows → csvfiles/05-01-2026/18.35.csv\n",
      "[OK] 05-01-2026 19.05: 2000 rows → csvfiles/05-01-2026/19.05.csv\n",
      "[OK] 05-01-2026 19.35: 2000 rows → csvfiles/05-01-2026/19.35.csv\n",
      "[SKIP] 05-01-2026 20.05\n",
      "[OK] 05-01-2026 20.35: 2000 rows → csvfiles/05-01-2026/20.35.csv\n",
      "[OK] 05-01-2026 21.05: 2000 rows → csvfiles/05-01-2026/21.05.csv\n",
      "[OK] 05-01-2026 21.35: 2000 rows → csvfiles/05-01-2026/21.35.csv\n",
      "[OK] 05-01-2026 22.05: 2000 rows → csvfiles/05-01-2026/22.05.csv\n",
      "[OK] 05-01-2026 22.35: 2000 rows → csvfiles/05-01-2026/22.35.csv\n",
      "[SKIP] 05-01-2026 23.05\n",
      "[OK] 05-01-2026 23.35: 2000 rows → csvfiles/05-01-2026/23.35.csv\n",
      "\n",
      "=== Processing 06-01-2026 ===\n",
      "[OK] 06-01-2026 00.05: 2000 rows → csvfiles/06-01-2026/00.05.csv\n",
      "[OK] 06-01-2026 00.35: 2000 rows → csvfiles/06-01-2026/00.35.csv\n",
      "[OK] 06-01-2026 01.05: 2000 rows → csvfiles/06-01-2026/01.05.csv\n",
      "[OK] 06-01-2026 01.35: 2000 rows → csvfiles/06-01-2026/01.35.csv\n",
      "[OK] 06-01-2026 02.05: 2000 rows → csvfiles/06-01-2026/02.05.csv\n",
      "[OK] 06-01-2026 02.35: 2000 rows → csvfiles/06-01-2026/02.35.csv\n",
      "[OK] 06-01-2026 03.05: 2000 rows → csvfiles/06-01-2026/03.05.csv\n",
      "[OK] 06-01-2026 03.35: 2000 rows → csvfiles/06-01-2026/03.35.csv\n",
      "[OK] 06-01-2026 04.05: 2000 rows → csvfiles/06-01-2026/04.05.csv\n",
      "[OK] 06-01-2026 04.35: 2000 rows → csvfiles/06-01-2026/04.35.csv\n",
      "[OK] 06-01-2026 05.05: 2000 rows → csvfiles/06-01-2026/05.05.csv\n",
      "[OK] 06-01-2026 05.35: 2000 rows → csvfiles/06-01-2026/05.35.csv\n",
      "[OK] 06-01-2026 06.05: 2000 rows → csvfiles/06-01-2026/06.05.csv\n",
      "[OK] 06-01-2026 06.35: 2000 rows → csvfiles/06-01-2026/06.35.csv\n",
      "[SKIP] 06-01-2026 07.05\n",
      "[OK] 06-01-2026 07.35: 2000 rows → csvfiles/06-01-2026/07.35.csv\n",
      "[OK] 06-01-2026 08.05: 2000 rows → csvfiles/06-01-2026/08.05.csv\n",
      "[OK] 06-01-2026 08.35: 2000 rows → csvfiles/06-01-2026/08.35.csv\n",
      "[OK] 06-01-2026 09.05: 2000 rows → csvfiles/06-01-2026/09.05.csv\n",
      "[OK] 06-01-2026 09.35: 2000 rows → csvfiles/06-01-2026/09.35.csv\n",
      "[OK] 06-01-2026 10.05: 2000 rows → csvfiles/06-01-2026/10.05.csv\n",
      "[OK] 06-01-2026 10.35: 2000 rows → csvfiles/06-01-2026/10.35.csv\n",
      "[OK] 06-01-2026 11.05: 2000 rows → csvfiles/06-01-2026/11.05.csv\n",
      "[OK] 06-01-2026 11.35: 2000 rows → csvfiles/06-01-2026/11.35.csv\n",
      "[OK] 06-01-2026 12.05: 2000 rows → csvfiles/06-01-2026/12.05.csv\n",
      "[SKIP] 06-01-2026 12.35\n",
      "[SKIP] 06-01-2026 13.05\n",
      "[OK] 06-01-2026 13.35: 2000 rows → csvfiles/06-01-2026/13.35.csv\n",
      "[OK] 06-01-2026 14.05: 2000 rows → csvfiles/06-01-2026/14.05.csv\n",
      "[OK] 06-01-2026 14.35: 2000 rows → csvfiles/06-01-2026/14.35.csv\n",
      "[OK] 06-01-2026 15.05: 2000 rows → csvfiles/06-01-2026/15.05.csv\n",
      "[OK] 06-01-2026 15.35: 2000 rows → csvfiles/06-01-2026/15.35.csv\n",
      "[OK] 06-01-2026 16.05: 2000 rows → csvfiles/06-01-2026/16.05.csv\n",
      "[OK] 06-01-2026 16.35: 2000 rows → csvfiles/06-01-2026/16.35.csv\n",
      "[OK] 06-01-2026 17.05: 2000 rows → csvfiles/06-01-2026/17.05.csv\n",
      "[OK] 06-01-2026 17.35: 2000 rows → csvfiles/06-01-2026/17.35.csv\n",
      "[OK] 06-01-2026 18.05: 2000 rows → csvfiles/06-01-2026/18.05.csv\n",
      "[OK] 06-01-2026 18.35: 2000 rows → csvfiles/06-01-2026/18.35.csv\n",
      "[OK] 06-01-2026 19.05: 2000 rows → csvfiles/06-01-2026/19.05.csv\n",
      "[OK] 06-01-2026 19.35: 2000 rows → csvfiles/06-01-2026/19.35.csv\n",
      "[OK] 06-01-2026 20.05: 2000 rows → csvfiles/06-01-2026/20.05.csv\n",
      "[OK] 06-01-2026 20.35: 2000 rows → csvfiles/06-01-2026/20.35.csv\n",
      "[OK] 06-01-2026 21.05: 2000 rows → csvfiles/06-01-2026/21.05.csv\n",
      "[OK] 06-01-2026 21.35: 2000 rows → csvfiles/06-01-2026/21.35.csv\n",
      "[OK] 06-01-2026 22.05: 2000 rows → csvfiles/06-01-2026/22.05.csv\n",
      "[OK] 06-01-2026 22.35: 2000 rows → csvfiles/06-01-2026/22.35.csv\n",
      "[OK] 06-01-2026 23.05: 2000 rows → csvfiles/06-01-2026/23.05.csv\n",
      "[OK] 06-01-2026 23.35: 2000 rows → csvfiles/06-01-2026/23.35.csv\n",
      "\n",
      "=== Processing 07-01-2026 ===\n",
      "[OK] 07-01-2026 00.05: 2000 rows → csvfiles/07-01-2026/00.05.csv\n",
      "[OK] 07-01-2026 00.35: 2000 rows → csvfiles/07-01-2026/00.35.csv\n",
      "[OK] 07-01-2026 01.05: 2000 rows → csvfiles/07-01-2026/01.05.csv\n",
      "[OK] 07-01-2026 01.35: 2000 rows → csvfiles/07-01-2026/01.35.csv\n",
      "[OK] 07-01-2026 02.05: 2000 rows → csvfiles/07-01-2026/02.05.csv\n",
      "[OK] 07-01-2026 02.35: 2000 rows → csvfiles/07-01-2026/02.35.csv\n",
      "[OK] 07-01-2026 03.05: 2000 rows → csvfiles/07-01-2026/03.05.csv\n",
      "[OK] 07-01-2026 03.35: 2000 rows → csvfiles/07-01-2026/03.35.csv\n",
      "[OK] 07-01-2026 04.05: 2000 rows → csvfiles/07-01-2026/04.05.csv\n",
      "[OK] 07-01-2026 04.35: 2000 rows → csvfiles/07-01-2026/04.35.csv\n",
      "[OK] 07-01-2026 05.05: 2000 rows → csvfiles/07-01-2026/05.05.csv\n",
      "[OK] 07-01-2026 05.35: 2000 rows → csvfiles/07-01-2026/05.35.csv\n",
      "[OK] 07-01-2026 06.05: 2000 rows → csvfiles/07-01-2026/06.05.csv\n",
      "[OK] 07-01-2026 06.35: 2000 rows → csvfiles/07-01-2026/06.35.csv\n",
      "[OK] 07-01-2026 07.05: 2000 rows → csvfiles/07-01-2026/07.05.csv\n",
      "[OK] 07-01-2026 07.35: 2000 rows → csvfiles/07-01-2026/07.35.csv\n",
      "[OK] 07-01-2026 08.05: 2000 rows → csvfiles/07-01-2026/08.05.csv\n",
      "[OK] 07-01-2026 08.35: 2000 rows → csvfiles/07-01-2026/08.35.csv\n",
      "[OK] 07-01-2026 09.05: 2000 rows → csvfiles/07-01-2026/09.05.csv\n",
      "[OK] 07-01-2026 09.35: 2000 rows → csvfiles/07-01-2026/09.35.csv\n",
      "[OK] 07-01-2026 10.05: 2000 rows → csvfiles/07-01-2026/10.05.csv\n",
      "[OK] 07-01-2026 10.35: 2000 rows → csvfiles/07-01-2026/10.35.csv\n",
      "[OK] 07-01-2026 11.05: 2000 rows → csvfiles/07-01-2026/11.05.csv\n",
      "[OK] 07-01-2026 11.35: 2000 rows → csvfiles/07-01-2026/11.35.csv\n",
      "[OK] 07-01-2026 12.05: 2000 rows → csvfiles/07-01-2026/12.05.csv\n",
      "[OK] 07-01-2026 12.35: 2000 rows → csvfiles/07-01-2026/12.35.csv\n",
      "[OK] 07-01-2026 13.05: 2000 rows → csvfiles/07-01-2026/13.05.csv\n",
      "[OK] 07-01-2026 13.35: 2000 rows → csvfiles/07-01-2026/13.35.csv\n",
      "[OK] 07-01-2026 14.05: 2000 rows → csvfiles/07-01-2026/14.05.csv\n",
      "[OK] 07-01-2026 14.35: 2000 rows → csvfiles/07-01-2026/14.35.csv\n",
      "[OK] 07-01-2026 15.05: 2000 rows → csvfiles/07-01-2026/15.05.csv\n",
      "[OK] 07-01-2026 15.35: 2000 rows → csvfiles/07-01-2026/15.35.csv\n",
      "[SKIP] 07-01-2026 16.05\n",
      "[OK] 07-01-2026 16.35: 2000 rows → csvfiles/07-01-2026/16.35.csv\n",
      "[OK] 07-01-2026 17.05: 2000 rows → csvfiles/07-01-2026/17.05.csv\n",
      "[OK] 07-01-2026 17.35: 2000 rows → csvfiles/07-01-2026/17.35.csv\n",
      "[OK] 07-01-2026 18.05: 2000 rows → csvfiles/07-01-2026/18.05.csv\n",
      "[OK] 07-01-2026 18.35: 2000 rows → csvfiles/07-01-2026/18.35.csv\n",
      "[OK] 07-01-2026 19.05: 2000 rows → csvfiles/07-01-2026/19.05.csv\n",
      "[OK] 07-01-2026 19.35: 2000 rows → csvfiles/07-01-2026/19.35.csv\n",
      "[OK] 07-01-2026 20.05: 2000 rows → csvfiles/07-01-2026/20.05.csv\n",
      "[OK] 07-01-2026 20.35: 2000 rows → csvfiles/07-01-2026/20.35.csv\n",
      "[OK] 07-01-2026 21.05: 2000 rows → csvfiles/07-01-2026/21.05.csv\n",
      "[OK] 07-01-2026 21.35: 2000 rows → csvfiles/07-01-2026/21.35.csv\n",
      "[OK] 07-01-2026 22.05: 2000 rows → csvfiles/07-01-2026/22.05.csv\n",
      "[OK] 07-01-2026 22.35: 2000 rows → csvfiles/07-01-2026/22.35.csv\n",
      "[OK] 07-01-2026 23.05: 2000 rows → csvfiles/07-01-2026/23.05.csv\n",
      "[OK] 07-01-2026 23.35: 2000 rows → csvfiles/07-01-2026/23.35.csv\n",
      "\n",
      "=== Processing 08-01-2026 ===\n",
      "[SKIP] 08-01-2026 00.05\n",
      "[SKIP] 08-01-2026 00.35\n",
      "[OK] 08-01-2026 01.05: 2000 rows → csvfiles/08-01-2026/01.05.csv\n",
      "[OK] 08-01-2026 01.35: 2000 rows → csvfiles/08-01-2026/01.35.csv\n",
      "[OK] 08-01-2026 02.05: 2000 rows → csvfiles/08-01-2026/02.05.csv\n",
      "[OK] 08-01-2026 02.35: 2000 rows → csvfiles/08-01-2026/02.35.csv\n",
      "[OK] 08-01-2026 03.05: 2000 rows → csvfiles/08-01-2026/03.05.csv\n",
      "[OK] 08-01-2026 03.35: 2000 rows → csvfiles/08-01-2026/03.35.csv\n",
      "[OK] 08-01-2026 04.05: 2000 rows → csvfiles/08-01-2026/04.05.csv\n",
      "[OK] 08-01-2026 04.35: 2000 rows → csvfiles/08-01-2026/04.35.csv\n",
      "[OK] 08-01-2026 05.05: 2000 rows → csvfiles/08-01-2026/05.05.csv\n",
      "[OK] 08-01-2026 05.35: 2000 rows → csvfiles/08-01-2026/05.35.csv\n",
      "[OK] 08-01-2026 06.05: 2000 rows → csvfiles/08-01-2026/06.05.csv\n",
      "[OK] 08-01-2026 06.35: 2000 rows → csvfiles/08-01-2026/06.35.csv\n",
      "[OK] 08-01-2026 07.05: 2000 rows → csvfiles/08-01-2026/07.05.csv\n",
      "[OK] 08-01-2026 07.35: 2000 rows → csvfiles/08-01-2026/07.35.csv\n",
      "[OK] 08-01-2026 08.05: 2000 rows → csvfiles/08-01-2026/08.05.csv\n",
      "[OK] 08-01-2026 08.35: 2000 rows → csvfiles/08-01-2026/08.35.csv\n",
      "[OK] 08-01-2026 09.05: 2000 rows → csvfiles/08-01-2026/09.05.csv\n",
      "[OK] 08-01-2026 09.35: 2000 rows → csvfiles/08-01-2026/09.35.csv\n",
      "[OK] 08-01-2026 10.05: 2000 rows → csvfiles/08-01-2026/10.05.csv\n",
      "[OK] 08-01-2026 10.35: 2000 rows → csvfiles/08-01-2026/10.35.csv\n",
      "[OK] 08-01-2026 11.05: 2000 rows → csvfiles/08-01-2026/11.05.csv\n",
      "[OK] 08-01-2026 11.35: 2000 rows → csvfiles/08-01-2026/11.35.csv\n",
      "[OK] 08-01-2026 12.05: 2000 rows → csvfiles/08-01-2026/12.05.csv\n",
      "[OK] 08-01-2026 12.35: 2000 rows → csvfiles/08-01-2026/12.35.csv\n",
      "[OK] 08-01-2026 13.05: 2000 rows → csvfiles/08-01-2026/13.05.csv\n",
      "[OK] 08-01-2026 13.35: 2000 rows → csvfiles/08-01-2026/13.35.csv\n",
      "[OK] 08-01-2026 14.05: 2000 rows → csvfiles/08-01-2026/14.05.csv\n",
      "[OK] 08-01-2026 14.35: 2000 rows → csvfiles/08-01-2026/14.35.csv\n",
      "[OK] 08-01-2026 15.05: 2000 rows → csvfiles/08-01-2026/15.05.csv\n",
      "[OK] 08-01-2026 15.35: 2000 rows → csvfiles/08-01-2026/15.35.csv\n",
      "[OK] 08-01-2026 16.05: 2000 rows → csvfiles/08-01-2026/16.05.csv\n",
      "[OK] 08-01-2026 16.35: 2000 rows → csvfiles/08-01-2026/16.35.csv\n",
      "[OK] 08-01-2026 17.05: 2000 rows → csvfiles/08-01-2026/17.05.csv\n",
      "[OK] 08-01-2026 17.35: 2000 rows → csvfiles/08-01-2026/17.35.csv\n",
      "[OK] 08-01-2026 18.05: 2000 rows → csvfiles/08-01-2026/18.05.csv\n",
      "[OK] 08-01-2026 18.35: 2000 rows → csvfiles/08-01-2026/18.35.csv\n",
      "[OK] 08-01-2026 19.05: 2000 rows → csvfiles/08-01-2026/19.05.csv\n",
      "[OK] 08-01-2026 19.35: 2000 rows → csvfiles/08-01-2026/19.35.csv\n",
      "[OK] 08-01-2026 20.05: 2000 rows → csvfiles/08-01-2026/20.05.csv\n",
      "[OK] 08-01-2026 20.35: 2000 rows → csvfiles/08-01-2026/20.35.csv\n",
      "[OK] 08-01-2026 21.05: 2000 rows → csvfiles/08-01-2026/21.05.csv\n",
      "[OK] 08-01-2026 21.35: 2000 rows → csvfiles/08-01-2026/21.35.csv\n",
      "[OK] 08-01-2026 22.05: 2000 rows → csvfiles/08-01-2026/22.05.csv\n",
      "[OK] 08-01-2026 22.35: 2000 rows → csvfiles/08-01-2026/22.35.csv\n",
      "[OK] 08-01-2026 23.05: 2000 rows → csvfiles/08-01-2026/23.05.csv\n",
      "[OK] 08-01-2026 23.35: 2000 rows → csvfiles/08-01-2026/23.35.csv\n",
      "\n",
      "=== Processing 09-01-2026 ===\n",
      "[OK] 09-01-2026 00.05: 2000 rows → csvfiles/09-01-2026/00.05.csv\n",
      "[OK] 09-01-2026 00.35: 2000 rows → csvfiles/09-01-2026/00.35.csv\n",
      "[OK] 09-01-2026 01.05: 2000 rows → csvfiles/09-01-2026/01.05.csv\n",
      "[OK] 09-01-2026 01.35: 2000 rows → csvfiles/09-01-2026/01.35.csv\n",
      "[OK] 09-01-2026 02.05: 2000 rows → csvfiles/09-01-2026/02.05.csv\n",
      "[OK] 09-01-2026 02.35: 2000 rows → csvfiles/09-01-2026/02.35.csv\n",
      "[OK] 09-01-2026 03.05: 2000 rows → csvfiles/09-01-2026/03.05.csv\n",
      "[OK] 09-01-2026 03.35: 2000 rows → csvfiles/09-01-2026/03.35.csv\n",
      "[OK] 09-01-2026 04.05: 2000 rows → csvfiles/09-01-2026/04.05.csv\n",
      "[OK] 09-01-2026 04.35: 2000 rows → csvfiles/09-01-2026/04.35.csv\n",
      "[OK] 09-01-2026 05.05: 2000 rows → csvfiles/09-01-2026/05.05.csv\n",
      "[OK] 09-01-2026 05.35: 2000 rows → csvfiles/09-01-2026/05.35.csv\n",
      "[OK] 09-01-2026 06.05: 2000 rows → csvfiles/09-01-2026/06.05.csv\n",
      "[OK] 09-01-2026 06.35: 2000 rows → csvfiles/09-01-2026/06.35.csv\n",
      "[SKIP] 09-01-2026 07.05\n",
      "[OK] 09-01-2026 07.35: 2000 rows → csvfiles/09-01-2026/07.35.csv\n",
      "[OK] 09-01-2026 08.05: 2000 rows → csvfiles/09-01-2026/08.05.csv\n",
      "[OK] 09-01-2026 08.35: 2000 rows → csvfiles/09-01-2026/08.35.csv\n",
      "[OK] 09-01-2026 09.05: 2000 rows → csvfiles/09-01-2026/09.05.csv\n",
      "[OK] 09-01-2026 09.35: 2000 rows → csvfiles/09-01-2026/09.35.csv\n",
      "[OK] 09-01-2026 10.05: 2000 rows → csvfiles/09-01-2026/10.05.csv\n",
      "[OK] 09-01-2026 10.35: 2000 rows → csvfiles/09-01-2026/10.35.csv\n",
      "[OK] 09-01-2026 11.05: 2000 rows → csvfiles/09-01-2026/11.05.csv\n",
      "[OK] 09-01-2026 11.35: 2000 rows → csvfiles/09-01-2026/11.35.csv\n",
      "[OK] 09-01-2026 12.05: 2000 rows → csvfiles/09-01-2026/12.05.csv\n",
      "[OK] 09-01-2026 12.35: 2000 rows → csvfiles/09-01-2026/12.35.csv\n",
      "[OK] 09-01-2026 13.05: 2000 rows → csvfiles/09-01-2026/13.05.csv\n",
      "[OK] 09-01-2026 13.35: 2000 rows → csvfiles/09-01-2026/13.35.csv\n",
      "[OK] 09-01-2026 14.05: 2000 rows → csvfiles/09-01-2026/14.05.csv\n",
      "[OK] 09-01-2026 14.35: 2000 rows → csvfiles/09-01-2026/14.35.csv\n",
      "[OK] 09-01-2026 15.05: 2000 rows → csvfiles/09-01-2026/15.05.csv\n",
      "[OK] 09-01-2026 15.35: 2000 rows → csvfiles/09-01-2026/15.35.csv\n",
      "[OK] 09-01-2026 16.05: 2000 rows → csvfiles/09-01-2026/16.05.csv\n",
      "[OK] 09-01-2026 16.35: 2000 rows → csvfiles/09-01-2026/16.35.csv\n",
      "[OK] 09-01-2026 17.05: 2000 rows → csvfiles/09-01-2026/17.05.csv\n",
      "[OK] 09-01-2026 17.35: 2000 rows → csvfiles/09-01-2026/17.35.csv\n",
      "[OK] 09-01-2026 18.05: 2000 rows → csvfiles/09-01-2026/18.05.csv\n",
      "[OK] 09-01-2026 18.35: 2000 rows → csvfiles/09-01-2026/18.35.csv\n",
      "[OK] 09-01-2026 19.05: 2000 rows → csvfiles/09-01-2026/19.05.csv\n",
      "[OK] 09-01-2026 19.35: 2000 rows → csvfiles/09-01-2026/19.35.csv\n",
      "[OK] 09-01-2026 20.05: 2000 rows → csvfiles/09-01-2026/20.05.csv\n",
      "[OK] 09-01-2026 20.35: 2000 rows → csvfiles/09-01-2026/20.35.csv\n",
      "[OK] 09-01-2026 21.05: 2000 rows → csvfiles/09-01-2026/21.05.csv\n",
      "[OK] 09-01-2026 21.35: 2000 rows → csvfiles/09-01-2026/21.35.csv\n",
      "[OK] 09-01-2026 22.05: 2000 rows → csvfiles/09-01-2026/22.05.csv\n",
      "[OK] 09-01-2026 22.35: 2000 rows → csvfiles/09-01-2026/22.35.csv\n",
      "[OK] 09-01-2026 23.05: 2000 rows → csvfiles/09-01-2026/23.05.csv\n",
      "[OK] 09-01-2026 23.35: 2000 rows → csvfiles/09-01-2026/23.35.csv\n",
      "\n",
      "=== Processing 12-01-2026 ===\n",
      "[OK] 12-01-2026 00.05: 2000 rows → csvfiles/12-01-2026/00.05.csv\n",
      "[OK] 12-01-2026 00.35: 2000 rows → csvfiles/12-01-2026/00.35.csv\n",
      "[OK] 12-01-2026 01.05: 2000 rows → csvfiles/12-01-2026/01.05.csv\n",
      "[OK] 12-01-2026 01.35: 2000 rows → csvfiles/12-01-2026/01.35.csv\n",
      "[OK] 12-01-2026 02.05: 2000 rows → csvfiles/12-01-2026/02.05.csv\n",
      "[OK] 12-01-2026 02.35: 2000 rows → csvfiles/12-01-2026/02.35.csv\n",
      "[OK] 12-01-2026 03.05: 2000 rows → csvfiles/12-01-2026/03.05.csv\n",
      "[OK] 12-01-2026 03.35: 2000 rows → csvfiles/12-01-2026/03.35.csv\n",
      "[OK] 12-01-2026 04.05: 2000 rows → csvfiles/12-01-2026/04.05.csv\n",
      "[OK] 12-01-2026 04.35: 2000 rows → csvfiles/12-01-2026/04.35.csv\n",
      "[OK] 12-01-2026 05.05: 2000 rows → csvfiles/12-01-2026/05.05.csv\n",
      "[OK] 12-01-2026 05.35: 2000 rows → csvfiles/12-01-2026/05.35.csv\n",
      "[OK] 12-01-2026 06.05: 2000 rows → csvfiles/12-01-2026/06.05.csv\n",
      "[OK] 12-01-2026 06.35: 2000 rows → csvfiles/12-01-2026/06.35.csv\n",
      "[OK] 12-01-2026 07.05: 2000 rows → csvfiles/12-01-2026/07.05.csv\n",
      "[OK] 12-01-2026 07.35: 2000 rows → csvfiles/12-01-2026/07.35.csv\n",
      "[OK] 12-01-2026 08.05: 2000 rows → csvfiles/12-01-2026/08.05.csv\n",
      "[OK] 12-01-2026 08.35: 2000 rows → csvfiles/12-01-2026/08.35.csv\n",
      "[OK] 12-01-2026 09.05: 2000 rows → csvfiles/12-01-2026/09.05.csv\n",
      "[OK] 12-01-2026 09.35: 2000 rows → csvfiles/12-01-2026/09.35.csv\n",
      "[OK] 12-01-2026 10.05: 2000 rows → csvfiles/12-01-2026/10.05.csv\n",
      "[OK] 12-01-2026 10.35: 2000 rows → csvfiles/12-01-2026/10.35.csv\n",
      "[OK] 12-01-2026 11.05: 2000 rows → csvfiles/12-01-2026/11.05.csv\n",
      "[OK] 12-01-2026 11.35: 2000 rows → csvfiles/12-01-2026/11.35.csv\n",
      "[OK] 12-01-2026 12.05: 2000 rows → csvfiles/12-01-2026/12.05.csv\n",
      "[OK] 12-01-2026 12.35: 2000 rows → csvfiles/12-01-2026/12.35.csv\n",
      "[OK] 12-01-2026 13.05: 2000 rows → csvfiles/12-01-2026/13.05.csv\n",
      "[OK] 12-01-2026 13.35: 2000 rows → csvfiles/12-01-2026/13.35.csv\n",
      "[OK] 12-01-2026 14.05: 2000 rows → csvfiles/12-01-2026/14.05.csv\n",
      "[SKIP] 12-01-2026 14.35\n",
      "[SKIP] 12-01-2026 15.05\n",
      "[SKIP] 12-01-2026 15.35\n",
      "[SKIP] 12-01-2026 16.05\n",
      "[SKIP] 12-01-2026 16.35\n",
      "[SKIP] 12-01-2026 17.05\n",
      "[SKIP] 12-01-2026 17.35\n",
      "[SKIP] 12-01-2026 18.05\n",
      "[SKIP] 12-01-2026 18.35\n",
      "[SKIP] 12-01-2026 19.05\n",
      "[SKIP] 12-01-2026 19.35\n",
      "[SKIP] 12-01-2026 20.05\n",
      "[SKIP] 12-01-2026 20.35\n",
      "[SKIP] 12-01-2026 21.05\n",
      "[SKIP] 12-01-2026 21.35\n",
      "[SKIP] 12-01-2026 22.05\n",
      "[SKIP] 12-01-2026 22.35\n",
      "[SKIP] 12-01-2026 23.05\n",
      "[SKIP] 12-01-2026 23.35\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import linregress\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG THE DAY (ALL you need to do in this block!!!)\n",
    "# -----------------------------\n",
    "DAYS = [\n",
    "    \"05-01-2026\",\n",
    "    \"06-01-2026\",\n",
    "    \"07-01-2026\",\n",
    "    \"08-01-2026\",\n",
    "    \"09-01-2026\",\n",
    "    \"12-01-2026\",\n",
    "]\n",
    "\n",
    "BASE_DIR = Path(\".\")          # root directory\n",
    "OUT_BASE = Path(\"csvfiles\")   # base output directory\n",
    "OUT_BASE.mkdir(exist_ok=True)\n",
    "\n",
    "def make_time_tags():\n",
    "    return [f\"{hh:02d}.{mm:02d}\" for hh in range(24) for mm in (5, 35)]\n",
    "float_re = re.compile(\n",
    "    r\"\"\"^[\\s+-]?(?:\\d+\\.?\\d*|\\.\\d+)(?:[eE][+-]?\\d+)?$\"\"\"\n",
    ")\n",
    "\n",
    "def is_number(tok: str) -> bool:\n",
    "    return bool(float_re.match(tok))\n",
    "\n",
    "def convert_one_time_tag(day: str, time_tag: str):\n",
    "    DAT_DIR = BASE_DIR / f\"{day}-DATfile\"\n",
    "    OUT_DIR = OUT_BASE / day\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    infile = DAT_DIR / f\"{time_tag}.dat\"\n",
    "    outfile = OUT_DIR / f\"{time_tag}.csv\"\n",
    "\n",
    "    if not infile.exists():\n",
    "        print(f\"[SKIP] {day} {time_tag}\")\n",
    "        return\n",
    "\n",
    "    rows = []\n",
    "    with infile.open(\"r\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            parts = line.split()\n",
    "\n",
    "            # Expect exactly 5 numeric columns\n",
    "            if len(parts) == 5 and all(is_number(p) for p in parts):\n",
    "                rows.append([float(p) for p in parts])\n",
    "\n",
    "    if not rows:\n",
    "        print(f\"[WARN] Empty {day} {time_tag}\")\n",
    "        return\n",
    "\n",
    "    with outfile.open(\"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\n",
    "            \"analog\",\n",
    "            \"analog_sterr\",\n",
    "            \"photon_counting\",\n",
    "            \"pc_sterr\",\n",
    "            \"overflow_info\",\n",
    "        ])\n",
    "        w.writerows(rows)\n",
    "\n",
    "    print(f\"[OK] {day} {time_tag}: {len(rows)} rows → {outfile}\")\n",
    "\n",
    "for day in DAYS:\n",
    "    print(f\"\\n=== Processing {day} ===\")\n",
    "    for TIME_TAG in make_time_tags():\n",
    "        convert_one_time_tag(day, TIME_TAG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecb5be2d-ce98-4f5c-9e1f-dfb3fd72534d",
   "metadata": {},
   "outputs": [],
   "source": [
    "AfterPulse = pd.read_csv(\"AfterPulse.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fe162a-b86f-4517-a4b1-a87abe5938fe",
   "metadata": {},
   "source": [
    "As you can see, all of the files in selected \"DAYS\" folder will be change to csv file automatically \n",
    "But you need to have raw (dat file) first!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe58712f-952e-40f2-804e-af96994e7b11",
   "metadata": {},
   "source": [
    "After that, you need to open csv file in order to automatically calculate Normalized Range Square (What we want)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df3de68-2809-40a2-86c3-dcad6fde49b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 05-01-2026: 92000 rows\n",
      "[OK] 06-01-2026: 90000 rows\n",
      "[OK] 07-01-2026: 94000 rows\n",
      "[OK] 08-01-2026: 92000 rows\n",
      "[OK] 09-01-2026: 94000 rows\n",
      "[OK] 12-01-2026: 58000 rows\n"
     ]
    }
   ],
   "source": [
    "CSV_BASE = Path(\"csvfiles\")\n",
    "\n",
    "day_dfs = {}\n",
    "\n",
    "for day in DAYS:\n",
    "    day_dir = CSV_BASE / day\n",
    "    if not day_dir.exists():\n",
    "        print(f\"[SKIP] {day} not found\")\n",
    "        continue\n",
    "\n",
    "    dfs = []\n",
    "    for csv_path in sorted(day_dir.glob(\"*.csv\")):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df[\"day\"] = day\n",
    "        df[\"time_tag\"] = csv_path.stem\n",
    "        dfs.append(df)\n",
    "\n",
    "    if dfs:\n",
    "        day_dfs[day] = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"[OK] {day}: {len(day_dfs[day])} rows\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1acdd450-5ad1-4bc1-b482-3b5dffcb11f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>analog</th>\n",
       "      <th>analog_sterr</th>\n",
       "      <th>photon_counting</th>\n",
       "      <th>pc_sterr</th>\n",
       "      <th>overflow_info</th>\n",
       "      <th>day</th>\n",
       "      <th>time_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28.9665</td>\n",
       "      <td>0.166826</td>\n",
       "      <td>176.1600</td>\n",
       "      <td>0.899132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67.2555</td>\n",
       "      <td>0.400356</td>\n",
       "      <td>195.0520</td>\n",
       "      <td>0.864446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>274.8830</td>\n",
       "      <td>2.351700</td>\n",
       "      <td>114.0360</td>\n",
       "      <td>1.378620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>473.2520</td>\n",
       "      <td>0.070407</td>\n",
       "      <td>1.2828</td>\n",
       "      <td>0.143848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>473.4570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     analog  analog_sterr  photon_counting  pc_sterr  overflow_info  \\\n",
       "0   28.9665      0.166826         176.1600  0.899132            0.0   \n",
       "1   67.2555      0.400356         195.0520  0.864446            0.0   \n",
       "2  274.8830      2.351700         114.0360  1.378620            0.0   \n",
       "3  473.2520      0.070407           1.2828  0.143848            0.0   \n",
       "4  473.4570      0.000000           0.0000  0.000000            0.0   \n",
       "\n",
       "          day time_tag  \n",
       "0  05-01-2026    00.05  \n",
       "1  05-01-2026    00.05  \n",
       "2  05-01-2026    00.05  \n",
       "3  05-01-2026    00.05  \n",
       "4  05-01-2026    00.05  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is what we have got (You can open other day in the day_dfs[\"dd-mm-yyyy\"] to see what's inside!) Cool!\n",
    "day_dfs[\"05-01-2026\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7390884-6be1-4a8b-85ab-6bb3922696e1",
   "metadata": {},
   "source": [
    "Next this is the parameters which you can change \"overlap_r1_m\" and \"overlap_r2_m\" (Changing others are unnecessary, but you can still try :))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63c55929-7f11-4d0c-b0af-a942484cc5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter\n",
    "#note: blend region shold be wider than overlap about 50 unit to make a smooth line\n",
    "config = {\n",
    "    \"bin_width_ns\": 25,\n",
    "    \"bin_spacing_m\": 3.75,\n",
    "    \"prf_hz\": 20,\n",
    "    \"dead_time_ns\": 3.06,\n",
    "    \"bg_start_m\": 3000,\n",
    "    \"bg_end_m\": 5621.25,\n",
    "    \"overlap_r1_m\": 200,\n",
    "    \"overlap_r2_m\": 300,\n",
    "    \"shift_search_bins\": 20,\n",
    "    \"afterpulse_provided\": True,\n",
    "    \"k_scale\": 0.064021849,\n",
    "    \"b_offset\": 0,\n",
    "}\n",
    "bin_spacing_m = 3.75\n",
    "range_m = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fd9ecd1-ae61-4815-9a83-fd516cbcb250",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_bin_and_snr_day(df, *, bin_spacing_m, config=None):\n",
    "    df = df.copy()\n",
    "\n",
    "    # per-timestamp bin index\n",
    "    df[\"bin_index\"] = df.groupby([\"day\", \"time_tag\"]).cumcount()\n",
    "    df[\"range_m\"] = df[\"bin_index\"] * bin_spacing_m\n",
    "\n",
    "    # SNR\n",
    "    df[\"SNR_analog\"] = df[\"analog\"] / df[\"analog_sterr\"]\n",
    "    df[\"SNR_Photon\"] = df[\"photon_counting\"] / df[\"pc_sterr\"]\n",
    "\n",
    "    # photon per bin\n",
    "    if config is not None:\n",
    "        df[\"photon_per_bin\"] = (\n",
    "            df[\"photon_counting\"] * config[\"bin_width_ns\"] * 1e-3\n",
    "        )\n",
    "\n",
    "    # ---- reorder columns ----\n",
    "    main_cols = [\n",
    "        \"day\",\n",
    "        \"time_tag\",\n",
    "        \"bin_index\",\n",
    "        \"range_m\",\n",
    "        \"analog\",\n",
    "        \"analog_sterr\",\n",
    "        \"photon_counting\",\n",
    "        \"pc_sterr\",\n",
    "        \"overflow_info\",\n",
    "        \"SNR_analog\",\n",
    "        \"SNR_Photon\",\n",
    "        \"photon_per_bin\",\n",
    "    ]\n",
    "\n",
    "    # keep metadata columns (day, time_tag, timestamp, etc.)\n",
    "    meta_cols = [c for c in df.columns if c not in main_cols]\n",
    "\n",
    "    df = df[main_cols + meta_cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "#call function\n",
    "for day in list(day_dfs.keys()):\n",
    "    day_dfs[day] = add_bin_and_snr_day(\n",
    "        day_dfs[day],\n",
    "        bin_spacing_m=bin_spacing_m,\n",
    "        config=config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b4f9536-9d59-4eaa-a633-d478b156b5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] 05-01-2026: 92000 rows\n",
      "[OK] 06-01-2026: 90000 rows\n",
      "[OK] 07-01-2026: 94000 rows\n",
      "[OK] 08-01-2026: 92000 rows\n",
      "[OK] 09-01-2026: 94000 rows\n",
      "[OK] 12-01-2026: 58000 rows\n",
      "\n",
      "Done. Example columns:\n",
      "05-01-2026 ['day', 'time_tag', 'bin_index', 'range_m', 'analog', 'analog_sterr', 'photon_counting', 'pc_sterr', 'overflow_info', 'SNR_analog', 'SNR_Photon', 'photon_per_bin', 'analog_bg_corr', 'Photon_per_bin_bg_corr', 'afterpulse_raw', 'afterpulse_counts_per_bin', 'photon_APcorr_counts', 'photon_deadtime_counts', 'photon_deadtime_corr']\n"
     ]
    }
   ],
   "source": [
    "# FULL PIPELINE (per-day, per-timestamp)\n",
    "# 1) Load CSVs into day_dfs\n",
    "# 2) Add bin_index, range_m, SNR, photon_per_bin\n",
    "# 3) Add background correction + afterpulse + deadtime correction\n",
    "# 4) (Optional) Reorder columns\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------------\n",
    "# CONFIG YOU MUST HAVE\n",
    "# -----------------------------\n",
    "# DAYS = [\"05-01-2026\", \"06-01-2026\", \"07-01-2026\"]\n",
    "# config = {\n",
    "#   \"bin_width_ns\": ...,\n",
    "#   \"dead_time_ns\": ...,\n",
    "#   \"bg_start_m\": ...,\n",
    "#   \"bg_end_m\": ...,\n",
    "# }\n",
    "# bin_spacing_m = 3.75\n",
    "#\n",
    "# AfterPulse (optional):\n",
    "# - DataFrame with column \"afterpulse_raw\"\n",
    "#   AfterPulse = pd.read_csv(\"afterpulse.csv\") or similar\n",
    "\n",
    "CSV_BASE = Path(\"csvfiles\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# HELPERS\n",
    "# -----------------------------\n",
    "def add_bin_and_snr_day(df, *, bin_spacing_m, config=None):\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    # bin_index resets per timestamp\n",
    "    df[\"bin_index\"] = df.groupby([\"day\", \"time_tag\"]).cumcount()\n",
    "    df[\"range_m\"] = df[\"bin_index\"] * bin_spacing_m\n",
    "\n",
    "    df[\"SNR_analog\"] = df[\"analog\"] / df[\"analog_sterr\"]\n",
    "    df[\"SNR_Photon\"] = df[\"photon_counting\"] / df[\"pc_sterr\"]\n",
    "\n",
    "    if config is not None:\n",
    "        df[\"photon_per_bin\"] = df[\"photon_counting\"] * config[\"bin_width_ns\"] * 1e-3\n",
    "\n",
    "    # (optional) reorder core columns to the front\n",
    "    main_cols = [\n",
    "        \"day\",\n",
    "        \"time_tag\",\n",
    "        \"bin_index\",\n",
    "        \"range_m\",\n",
    "        \"analog\",\n",
    "        \"analog_sterr\",\n",
    "        \"photon_counting\",\n",
    "        \"pc_sterr\",\n",
    "        \"overflow_info\",\n",
    "        \"SNR_analog\",\n",
    "        \"SNR_Photon\",\n",
    "        \"photon_per_bin\",\n",
    "    ]\n",
    "    \n",
    "    keep_cols = [c for c in df.columns if c not in main_cols]\n",
    "    df = df[[c for c in main_cols if c in df.columns] + keep_cols]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_bg_afterpulse_deadtime_day(df, *, config, AfterPulse=None):\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    required = [\"range_m\", \"analog\", \"photon_per_bin\", \"day\", \"time_tag\"]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"add_bg_afterpulse_deadtime_day: missing columns {missing}\")\n",
    "\n",
    "    bin_width_s = config[\"bin_width_ns\"] * 1e-9\n",
    "    dead_time_s = config[\"dead_time_ns\"] * 1e-9\n",
    "\n",
    "    # create columns (safe defaults)\n",
    "    for col in [\n",
    "        \"analog_bg_corr\",\n",
    "        \"Photon_per_bin_bg_corr\",\n",
    "        \"afterpulse_raw\",\n",
    "        \"afterpulse_counts_per_bin\",\n",
    "        \"photon_APcorr_counts\",\n",
    "        \"photon_deadtime_counts\",\n",
    "        \"photon_deadtime_corr\",\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # Preload afterpulse raw array if provided\n",
    "    ap_raw = None\n",
    "    if AfterPulse is not None:\n",
    "        if isinstance(AfterPulse, pd.DataFrame):\n",
    "            if \"afterpulse_raw\" not in AfterPulse.columns:\n",
    "                raise KeyError(\"AfterPulse DataFrame must have column 'afterpulse_raw'\")\n",
    "            ap_raw = AfterPulse[\"afterpulse_raw\"].to_numpy()\n",
    "        else:\n",
    "            # allow passing a 1D array-like\n",
    "            ap_raw = np.asarray(AfterPulse)\n",
    "\n",
    "    # process per timestamp group\n",
    "    for (day, time_tag), g_idx in df.groupby([\"day\", \"time_tag\"]).groups.items():\n",
    "        g = df.loc[g_idx]\n",
    "\n",
    "        bg_mask = (g[\"range_m\"] >= config[\"bg_start_m\"]) & (g[\"range_m\"] <= config[\"bg_end_m\"])\n",
    "        bg_row = g.loc[bg_mask]\n",
    "        if bg_row.empty:\n",
    "            # no bg region for this timestamp\n",
    "            continue\n",
    "\n",
    "        analog_bg_mean = bg_row[\"analog\"].mean()\n",
    "        photon_bg_mean = bg_row[\"photon_per_bin\"].mean()\n",
    "\n",
    "        df.loc[g_idx, \"analog_bg_corr\"] = g[\"analog\"] - analog_bg_mean\n",
    "        df.loc[g_idx, \"Photon_per_bin_bg_corr\"] = g[\"photon_per_bin\"] - photon_bg_mean\n",
    "\n",
    "        # Afterpulse correction\n",
    "        if ap_raw is not None:\n",
    "            if \"bin_index\" not in g.columns:\n",
    "                raise KeyError(\"Need 'bin_index' to align AfterPulse by bin. Run add_bin_and_snr_day first.\")\n",
    "\n",
    "            bi = g[\"bin_index\"].to_numpy().astype(int)\n",
    "            valid = (bi >= 0) & (bi < len(ap_raw))\n",
    "\n",
    "            ap_vals = np.full(len(g), np.nan)\n",
    "            ap_vals[valid] = ap_raw[bi[valid]]\n",
    "\n",
    "            df.loc[g_idx, \"afterpulse_raw\"] = ap_vals\n",
    "            df.loc[g_idx, \"afterpulse_counts_per_bin\"] = ap_vals * config[\"bin_width_ns\"] * 1e-3\n",
    "\n",
    "            df.loc[g_idx, \"photon_APcorr_counts\"] = (\n",
    "                df.loc[g_idx, \"Photon_per_bin_bg_corr\"] - df.loc[g_idx, \"afterpulse_counts_per_bin\"]\n",
    "            )\n",
    "        else:\n",
    "            # no afterpulse profile => AP-corrected = bg-corrected\n",
    "            df.loc[g_idx, \"photon_APcorr_counts\"] = df.loc[g_idx, \"Photon_per_bin_bg_corr\"]\n",
    "\n",
    "        # Deadtime correction\n",
    "        x = df.loc[g_idx, \"photon_APcorr_counts\"].to_numpy()\n",
    "\n",
    "        # stage 1\n",
    "        photon_deadtime_counts = x / (1 - x * dead_time_s / bin_width_s)\n",
    "\n",
    "        # validity\n",
    "        ratio = photon_deadtime_counts * dead_time_s / bin_width_s\n",
    "\n",
    "        photon_deadtime_corr = np.where(\n",
    "            ratio < 1,\n",
    "            photon_deadtime_counts / (1 - ratio),\n",
    "            np.nan\n",
    "        )\n",
    "\n",
    "        df.loc[g_idx, \"photon_deadtime_counts\"] = photon_deadtime_counts\n",
    "        df.loc[g_idx, \"photon_deadtime_corr\"] = photon_deadtime_corr\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 1) LOAD CSVs INTO day_dfs\n",
    "# -----------------------------\n",
    "day_dfs = {}\n",
    "\n",
    "for day in DAYS:\n",
    "    day_dir = CSV_BASE / day\n",
    "    if not day_dir.exists():\n",
    "        print(f\"[SKIP] {day} not found\")\n",
    "        continue\n",
    "\n",
    "    dfs = []\n",
    "    for csv_path in sorted(day_dir.glob(\"*.csv\")):\n",
    "        df = pd.read_csv(csv_path)\n",
    "        df.columns = df.columns.str.strip()\n",
    "        df[\"day\"] = day\n",
    "        df[\"time_tag\"] = csv_path.stem\n",
    "        dfs.append(df)\n",
    "\n",
    "    if dfs:\n",
    "        day_dfs[day] = pd.concat(dfs, ignore_index=True)\n",
    "        print(f\"[OK] {day}: {len(day_dfs[day])} rows\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 2) ADD BIN/RANGE/SNR/PHOTON_PER_BIN\n",
    "# 3) ADD BG + AFTERPULSE + DEADTIME CORRECTION\n",
    "# -----------------------------\n",
    "for day in list(day_dfs.keys()):\n",
    "    day_dfs[day] = add_bin_and_snr_day(\n",
    "        day_dfs[day],\n",
    "        bin_spacing_m=bin_spacing_m,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    day_dfs[day] = add_bg_afterpulse_deadtime_day(\n",
    "        day_dfs[day],\n",
    "        config=config,\n",
    "        AfterPulse=AfterPulse  # set to None if you don't have it\n",
    "    )\n",
    "\n",
    "print(\"\\nDone. Example columns:\")\n",
    "print(list(day_dfs.keys())[0], day_dfs[list(day_dfs.keys())[0]].columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22059c4c-51d2-4b47-b7a7-6bcd2e37a8f9",
   "metadata": {},
   "source": [
    "Check if this already added \"bin_index\", \"range_m\", \"SNR_analog\", \"SNR_Photon\", \"photon_per_bin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4383461-4a38-44c6-9b8e-0e7aa7647396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>time_tag</th>\n",
       "      <th>bin_index</th>\n",
       "      <th>range_m</th>\n",
       "      <th>analog</th>\n",
       "      <th>analog_sterr</th>\n",
       "      <th>photon_counting</th>\n",
       "      <th>pc_sterr</th>\n",
       "      <th>overflow_info</th>\n",
       "      <th>SNR_analog</th>\n",
       "      <th>SNR_Photon</th>\n",
       "      <th>photon_per_bin</th>\n",
       "      <th>analog_bg_corr</th>\n",
       "      <th>Photon_per_bin_bg_corr</th>\n",
       "      <th>afterpulse_raw</th>\n",
       "      <th>afterpulse_counts_per_bin</th>\n",
       "      <th>photon_APcorr_counts</th>\n",
       "      <th>photon_deadtime_counts</th>\n",
       "      <th>photon_deadtime_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>28.9665</td>\n",
       "      <td>0.166826</td>\n",
       "      <td>176.1600</td>\n",
       "      <td>0.899132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>173.633007</td>\n",
       "      <td>195.922289</td>\n",
       "      <td>4.40400</td>\n",
       "      <td>25.248878</td>\n",
       "      <td>4.397529</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.397529</td>\n",
       "      <td>9.523770</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>67.2555</td>\n",
       "      <td>0.400356</td>\n",
       "      <td>195.0520</td>\n",
       "      <td>0.864446</td>\n",
       "      <td>0.0</td>\n",
       "      <td>167.989240</td>\n",
       "      <td>225.638154</td>\n",
       "      <td>4.87630</td>\n",
       "      <td>63.537878</td>\n",
       "      <td>4.869829</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.869829</td>\n",
       "      <td>12.056033</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>2</td>\n",
       "      <td>7.50</td>\n",
       "      <td>274.8830</td>\n",
       "      <td>2.351700</td>\n",
       "      <td>114.0360</td>\n",
       "      <td>1.378620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>116.886933</td>\n",
       "      <td>82.717500</td>\n",
       "      <td>2.85090</td>\n",
       "      <td>271.165378</td>\n",
       "      <td>2.844429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.844429</td>\n",
       "      <td>4.363679</td>\n",
       "      <td>9.366416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>3</td>\n",
       "      <td>11.25</td>\n",
       "      <td>473.2520</td>\n",
       "      <td>0.070407</td>\n",
       "      <td>1.2828</td>\n",
       "      <td>0.143848</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6721.670745</td>\n",
       "      <td>8.917747</td>\n",
       "      <td>0.03207</td>\n",
       "      <td>469.534378</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025599</td>\n",
       "      <td>0.025679</td>\n",
       "      <td>0.025760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>05-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>4</td>\n",
       "      <td>15.00</td>\n",
       "      <td>473.4570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>469.739378</td>\n",
       "      <td>-0.006471</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.006471</td>\n",
       "      <td>-0.006466</td>\n",
       "      <td>-0.006461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          day time_tag  bin_index  range_m    analog  analog_sterr  \\\n",
       "0  05-01-2026    00.05          0     0.00   28.9665      0.166826   \n",
       "1  05-01-2026    00.05          1     3.75   67.2555      0.400356   \n",
       "2  05-01-2026    00.05          2     7.50  274.8830      2.351700   \n",
       "3  05-01-2026    00.05          3    11.25  473.2520      0.070407   \n",
       "4  05-01-2026    00.05          4    15.00  473.4570      0.000000   \n",
       "\n",
       "   photon_counting  pc_sterr  overflow_info   SNR_analog  SNR_Photon  \\\n",
       "0         176.1600  0.899132            0.0   173.633007  195.922289   \n",
       "1         195.0520  0.864446            0.0   167.989240  225.638154   \n",
       "2         114.0360  1.378620            0.0   116.886933   82.717500   \n",
       "3           1.2828  0.143848            0.0  6721.670745    8.917747   \n",
       "4           0.0000  0.000000            0.0          inf         NaN   \n",
       "\n",
       "   photon_per_bin  analog_bg_corr  Photon_per_bin_bg_corr  afterpulse_raw  \\\n",
       "0         4.40400       25.248878                4.397529             0.0   \n",
       "1         4.87630       63.537878                4.869829             0.0   \n",
       "2         2.85090      271.165378                2.844429             0.0   \n",
       "3         0.03207      469.534378                0.025599             0.0   \n",
       "4         0.00000      469.739378               -0.006471             0.0   \n",
       "\n",
       "   afterpulse_counts_per_bin  photon_APcorr_counts  photon_deadtime_counts  \\\n",
       "0                        0.0              4.397529                9.523770   \n",
       "1                        0.0              4.869829               12.056033   \n",
       "2                        0.0              2.844429                4.363679   \n",
       "3                        0.0              0.025599                0.025679   \n",
       "4                        0.0             -0.006471               -0.006466   \n",
       "\n",
       "   photon_deadtime_corr  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2              9.366416  \n",
       "3              0.025760  \n",
       "4             -0.006461  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_dfs[\"05-01-2026\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f75031ec-ef1a-42f8-a29d-afd91b1a73e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_glue_merge_day(df, *, config):\n",
    "    \"\"\"\n",
    "    For each (day, time_tag):\n",
    "      - compute k_scale & b_offset from overlap region regression\n",
    "      - compute analog_scaled_for_glue\n",
    "      - cosine blend weight_w\n",
    "      - merged_counts_per_bin, range2_corrected_counts, range2_norm\n",
    "      - overlap ratio mean/std (store as columns)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df.columns = df.columns.str.strip()\n",
    "\n",
    "    required = [\n",
    "        \"day\",\"time_tag\",\"range_m\",\n",
    "        \"analog_bg_corr\",\"Photon_per_bin_bg_corr\",\n",
    "        \"photon_APcorr_counts\",\n",
    "    ]\n",
    "    missing = [c for c in required if c not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"add_glue_merge_day: missing columns {missing}\")\n",
    "\n",
    "    # output columns\n",
    "    for col in [\n",
    "        \"k_scale\",\"b_offset\",\"r2_overlap\",\n",
    "        \"analog_scaled_for_glue\",\n",
    "        \"weight_w\",\n",
    "        \"merged_counts_per_bin\",\n",
    "        \"range2_corrected_counts\",\n",
    "        \"range2_norm\",\n",
    "        \"ratio_mean_overlap\",\n",
    "        \"ratio_std_overlap\",\n",
    "    ]:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "\n",
    "    # blend region\n",
    "    blend_r1_m = config[\"overlap_r1_m\"] - 100\n",
    "    blend_r2_m = config[\"overlap_r2_m\"] + 100\n",
    "\n",
    "    # per timestamp\n",
    "    for (day, time_tag), g_idx in df.groupby([\"day\",\"time_tag\"]).groups.items():\n",
    "        g = df.loc[g_idx]\n",
    "\n",
    "        # overlap mask for regression\n",
    "        m_overlap = (\n",
    "            (g[\"range_m\"] >= config[\"overlap_r1_m\"]) &\n",
    "            (g[\"range_m\"] <= config[\"overlap_r2_m\"])\n",
    "        )\n",
    "        go = g.loc[m_overlap]\n",
    "\n",
    "        if go.empty:\n",
    "            continue\n",
    "\n",
    "        # regression x=analog_bg_corr, y=Photon_per_bin_bg_corr\n",
    "        x = go[\"analog_bg_corr\"].to_numpy(dtype=float)\n",
    "        y = go[\"Photon_per_bin_bg_corr\"].to_numpy(dtype=float)\n",
    "\n",
    "        # drop NaNs\n",
    "        ok = np.isfinite(x) & np.isfinite(y)\n",
    "        if ok.sum() < 2:\n",
    "            continue\n",
    "\n",
    "        slope, intercept = np.polyfit(x[ok], y[ok], 1)  # slope=k_scale, intercept=b_offset\n",
    "\n",
    "        # R^2\n",
    "        yhat = slope * x[ok] + intercept\n",
    "        ss_res = np.sum((y[ok] - yhat) ** 2)\n",
    "        ss_tot = np.sum((y[ok] - np.mean(y[ok])) ** 2)\n",
    "        r2 = np.nan if ss_tot == 0 else 1 - ss_res / ss_tot\n",
    "\n",
    "        df.loc[g_idx, \"k_scale\"] = slope\n",
    "        df.loc[g_idx, \"b_offset\"] = intercept\n",
    "        df.loc[g_idx, \"r2_overlap\"] = r2\n",
    "\n",
    "        # scaled analog\n",
    "        df.loc[g_idx, \"analog_scaled_for_glue\"] = slope * g[\"analog_bg_corr\"] + intercept\n",
    "\n",
    "        # cosine blend weights\n",
    "        r = g[\"range_m\"].to_numpy(dtype=float)\n",
    "        w = np.zeros_like(r, dtype=float)\n",
    "        w[r > blend_r2_m] = 1.0\n",
    "        m = (r >= blend_r1_m) & (r <= blend_r2_m)\n",
    "        if blend_r2_m > blend_r1_m:\n",
    "            w[m] = 0.5 * (1.0 - np.cos(np.pi * (r[m] - blend_r1_m) / (blend_r2_m - blend_r1_m)))\n",
    "\n",
    "        df.loc[g_idx, \"weight_w\"] = w\n",
    "\n",
    "        # merged\n",
    "        df.loc[g_idx, \"merged_counts_per_bin\"] = (\n",
    "            (1.0 - w) * df.loc[g_idx, \"analog_scaled_for_glue\"].to_numpy(dtype=float)\n",
    "            + w * g[\"Photon_per_bin_bg_corr\"].to_numpy(dtype=float)\n",
    "        )\n",
    "\n",
    "        # range^2 corrected\n",
    "        df.loc[g_idx, \"range2_corrected_counts\"] = df.loc[g_idx, \"merged_counts_per_bin\"] * (r ** 2)\n",
    "\n",
    "        # normalized per timestamp\n",
    "        mx = np.nanmax(df.loc[g_idx, \"range2_corrected_counts\"].to_numpy(dtype=float))\n",
    "        if np.isfinite(mx) and mx != 0:\n",
    "            df.loc[g_idx, \"range2_norm\"] = df.loc[g_idx, \"range2_corrected_counts\"] / mx\n",
    "\n",
    "        # ratio stats in overlap region (use photon_APcorr_counts per your snippet)\n",
    "        denom = go[\"photon_APcorr_counts\"].to_numpy(dtype=float)\n",
    "        numer = (slope * go[\"analog_bg_corr\"].to_numpy(dtype=float) + intercept)\n",
    "\n",
    "        ok2 = np.isfinite(numer) & np.isfinite(denom) & (denom != 0)\n",
    "        if ok2.sum() > 0:\n",
    "            ratio = numer[ok2] / denom[ok2]\n",
    "            df.loc[g_idx, \"ratio_mean_overlap\"] = np.mean(ratio)\n",
    "            df.loc[g_idx, \"ratio_std_overlap\"] = np.std(ratio, ddof=1) if ok2.sum() > 1 else 0.0\n",
    "\n",
    "    return df\n",
    "\n",
    "for day in list(day_dfs.keys()):\n",
    "    day_dfs[day] = add_bin_and_snr_day(\n",
    "        day_dfs[day],\n",
    "        bin_spacing_m=bin_spacing_m,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    day_dfs[day] = add_bg_afterpulse_deadtime_day(\n",
    "        day_dfs[day],\n",
    "        config=config,\n",
    "        AfterPulse=AfterPulse\n",
    "    )\n",
    "\n",
    "    day_dfs[day] = add_glue_merge_day(\n",
    "        day_dfs[day],\n",
    "        config=config\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b8d84a3-2e5a-4417-8656-45df7c3f194e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>time_tag</th>\n",
       "      <th>bin_index</th>\n",
       "      <th>range_m</th>\n",
       "      <th>analog</th>\n",
       "      <th>analog_sterr</th>\n",
       "      <th>photon_counting</th>\n",
       "      <th>pc_sterr</th>\n",
       "      <th>overflow_info</th>\n",
       "      <th>SNR_analog</th>\n",
       "      <th>...</th>\n",
       "      <th>k_scale</th>\n",
       "      <th>b_offset</th>\n",
       "      <th>r2_overlap</th>\n",
       "      <th>analog_scaled_for_glue</th>\n",
       "      <th>weight_w</th>\n",
       "      <th>merged_counts_per_bin</th>\n",
       "      <th>range2_corrected_counts</th>\n",
       "      <th>range2_norm</th>\n",
       "      <th>ratio_mean_overlap</th>\n",
       "      <th>ratio_std_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>06-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>25.9671</td>\n",
       "      <td>0.137546</td>\n",
       "      <td>182.643000</td>\n",
       "      <td>0.874720</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.887885e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>4.081326</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>5.292899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.292899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.003005</td>\n",
       "      <td>0.054818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>06-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>1</td>\n",
       "      <td>3.75</td>\n",
       "      <td>60.1914</td>\n",
       "      <td>0.384231</td>\n",
       "      <td>197.266000</td>\n",
       "      <td>0.886297</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.566542e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>4.081326</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>7.162849</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.162849</td>\n",
       "      <td>100.727562</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>1.003005</td>\n",
       "      <td>0.054818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>06-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>2</td>\n",
       "      <td>7.50</td>\n",
       "      <td>276.9100</td>\n",
       "      <td>2.353310</td>\n",
       "      <td>102.843000</td>\n",
       "      <td>1.353450</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.176683e+02</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>4.081326</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>19.003938</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.003938</td>\n",
       "      <td>1068.971508</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>1.003005</td>\n",
       "      <td>0.054818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>06-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>3</td>\n",
       "      <td>11.25</td>\n",
       "      <td>473.4350</td>\n",
       "      <td>0.046501</td>\n",
       "      <td>0.350146</td>\n",
       "      <td>0.075929</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.018124e+04</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>4.081326</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>29.741687</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.741687</td>\n",
       "      <td>3764.182313</td>\n",
       "      <td>0.003683</td>\n",
       "      <td>1.003005</td>\n",
       "      <td>0.054818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>06-01-2026</td>\n",
       "      <td>00.05</td>\n",
       "      <td>4</td>\n",
       "      <td>15.00</td>\n",
       "      <td>473.4570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033347</td>\n",
       "      <td>0.033368</td>\n",
       "      <td>0.0</td>\n",
       "      <td>inf</td>\n",
       "      <td>...</td>\n",
       "      <td>0.054638</td>\n",
       "      <td>4.081326</td>\n",
       "      <td>0.567251</td>\n",
       "      <td>29.742889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.742889</td>\n",
       "      <td>6692.150125</td>\n",
       "      <td>0.006548</td>\n",
       "      <td>1.003005</td>\n",
       "      <td>0.054818</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          day time_tag  bin_index  range_m    analog  analog_sterr  \\\n",
       "0  06-01-2026    00.05          0     0.00   25.9671      0.137546   \n",
       "1  06-01-2026    00.05          1     3.75   60.1914      0.384231   \n",
       "2  06-01-2026    00.05          2     7.50  276.9100      2.353310   \n",
       "3  06-01-2026    00.05          3    11.25  473.4350      0.046501   \n",
       "4  06-01-2026    00.05          4    15.00  473.4570      0.000000   \n",
       "\n",
       "   photon_counting  pc_sterr  overflow_info    SNR_analog  ...   k_scale  \\\n",
       "0       182.643000  0.874720            0.0  1.887885e+02  ...  0.054638   \n",
       "1       197.266000  0.886297            0.0  1.566542e+02  ...  0.054638   \n",
       "2       102.843000  1.353450            0.0  1.176683e+02  ...  0.054638   \n",
       "3         0.350146  0.075929            0.0  1.018124e+04  ...  0.054638   \n",
       "4         0.033347  0.033368            0.0           inf  ...  0.054638   \n",
       "\n",
       "   b_offset  r2_overlap  analog_scaled_for_glue  weight_w  \\\n",
       "0  4.081326    0.567251                5.292899       0.0   \n",
       "1  4.081326    0.567251                7.162849       0.0   \n",
       "2  4.081326    0.567251               19.003938       0.0   \n",
       "3  4.081326    0.567251               29.741687       0.0   \n",
       "4  4.081326    0.567251               29.742889       0.0   \n",
       "\n",
       "   merged_counts_per_bin  range2_corrected_counts  range2_norm  \\\n",
       "0               5.292899                 0.000000     0.000000   \n",
       "1               7.162849               100.727562     0.000099   \n",
       "2              19.003938              1068.971508     0.001046   \n",
       "3              29.741687              3764.182313     0.003683   \n",
       "4              29.742889              6692.150125     0.006548   \n",
       "\n",
       "   ratio_mean_overlap  ratio_std_overlap  \n",
       "0            1.003005           0.054818  \n",
       "1            1.003005           0.054818  \n",
       "2            1.003005           0.054818  \n",
       "3            1.003005           0.054818  \n",
       "4            1.003005           0.054818  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "day_dfs[\"06-01-2026\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26f2febf-3f73-40af-b711-81f3efbd7053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           k_scale  b_offset  r2_overlap  ratio_mean_overlap  \\\n",
      "time_tag                                                       \n",
      "00.05     0.054638  4.081326    0.567251            1.003005   \n",
      "00.35     0.024351  4.211935    0.525145            1.003293   \n",
      "01.05     0.040368  4.131893    0.530272            1.003134   \n",
      "01.35     0.025485  4.211451    0.532614            1.002957   \n",
      "02.05     0.055378  4.085984    0.547350            1.003075   \n",
      "02.35     0.032008  4.179933    0.552977            1.002798   \n",
      "03.05     0.011399  4.266697    0.553474            1.003544   \n",
      "03.35     0.049286  4.113252    0.582343            1.002324   \n",
      "04.05     0.020569  4.200353    0.535081            1.003978   \n",
      "04.35     0.032789  4.208531    0.498197            1.002866   \n",
      "05.05     0.064737  4.095597    0.516126            1.002797   \n",
      "05.35     0.042347  4.115825    0.542776            1.003570   \n",
      "06.05     0.035134  4.163550    0.547317            1.002803   \n",
      "06.35     0.027590  4.194492    0.539814            1.003155   \n",
      "07.35     0.036382  3.251390    0.559656            1.003650   \n",
      "08.05     0.019435  1.877056    0.549235            1.009026   \n",
      "08.35     0.031323  1.260683    0.626036            1.009145   \n",
      "09.05     0.015993  0.977694    0.665022            1.012992   \n",
      "09.35     0.028371  0.326799    0.820048            1.013397   \n",
      "10.05     0.017124  0.567917    0.778257            1.011534   \n",
      "10.35     0.013448  0.427502    0.772566            1.013318   \n",
      "11.05     0.032048  0.200020    0.908104            1.005091   \n",
      "11.35     0.046761  0.132449    0.921391            1.007148   \n",
      "12.05     0.038323  0.204282    0.916568            1.008463   \n",
      "13.35     0.043713  0.184113    0.928293            1.013472   \n",
      "14.05     0.029966  0.240339    0.898661            1.010488   \n",
      "14.35     0.012852  0.377426    0.776432            1.014640   \n",
      "15.05     0.009948  0.452733    0.695236            1.016278   \n",
      "15.35     0.016692  0.500615    0.757848            1.014790   \n",
      "16.05     0.013983  0.810889    0.704863            1.013129   \n",
      "16.35     0.010403  1.115251    0.587214            1.016588   \n",
      "17.05     0.013413  1.493204    0.607038            1.011127   \n",
      "17.35     0.041008  2.064227    0.562689            1.005756   \n",
      "18.05     0.014064  3.863238    0.537003            1.004056   \n",
      "18.35     0.029462  4.220507    0.532302            1.002757   \n",
      "19.05     0.012398  4.271456    0.529809            1.003924   \n",
      "19.35     0.040309  4.168337    0.569298            1.001992   \n",
      "20.05     0.044532  4.171549    0.565346            1.002138   \n",
      "20.35     0.025416  4.246216    0.523987            1.002442   \n",
      "21.05     0.026260  4.245294    0.519103            1.002660   \n",
      "21.35     0.027650  4.233970    0.522414            1.002569   \n",
      "22.05     0.051934  4.131388    0.544475            1.002043   \n",
      "22.35     0.011886  4.344744    0.540216            1.002712   \n",
      "23.05     0.014944  4.322692    0.565065            1.002476   \n",
      "23.35     0.028598  4.220150    0.580600            1.002139   \n",
      "\n",
      "          ratio_std_overlap  \n",
      "time_tag                     \n",
      "00.05              0.054818  \n",
      "00.35              0.057934  \n",
      "01.05              0.056320  \n",
      "01.35              0.054745  \n",
      "02.05              0.055805  \n",
      "02.35              0.053016  \n",
      "03.05              0.060168  \n",
      "03.35              0.048334  \n",
      "04.05              0.063598  \n",
      "04.35              0.054011  \n",
      "05.05              0.053259  \n",
      "05.35              0.060234  \n",
      "06.05              0.053200  \n",
      "06.35              0.056486  \n",
      "07.35              0.060780  \n",
      "08.05              0.095245  \n",
      "08.35              0.093922  \n",
      "09.05              0.111752  \n",
      "09.35              0.103132  \n",
      "10.05              0.099073  \n",
      "10.35              0.107820  \n",
      "11.05              0.061437  \n",
      "11.35              0.067277  \n",
      "12.05              0.077486  \n",
      "13.35              0.096320  \n",
      "14.05              0.085567  \n",
      "14.35              0.115314  \n",
      "15.05              0.124209  \n",
      "15.35              0.113180  \n",
      "16.05              0.109073  \n",
      "16.35              0.128929  \n",
      "17.05              0.104719  \n",
      "17.35              0.075717  \n",
      "18.05              0.064195  \n",
      "18.35              0.052706  \n",
      "19.05              0.063301  \n",
      "19.35              0.044796  \n",
      "20.05              0.046397  \n",
      "20.35              0.049707  \n",
      "21.05              0.052008  \n",
      "21.35              0.051020  \n",
      "22.05              0.045402  \n",
      "22.35              0.052619  \n",
      "23.05              0.050018  \n",
      "23.35              0.046250  \n"
     ]
    }
   ],
   "source": [
    "day = \"06-01-2026\"\n",
    "\n",
    "summary = (\n",
    "    day_dfs[day]\n",
    "    .groupby(\"time_tag\")[[\"k_scale\",\"b_offset\",\"r2_overlap\",\"ratio_mean_overlap\",\"ratio_std_overlap\"]]\n",
    "    .first()\n",
    ")\n",
    "\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d94c0a05-5d9a-4f87-b68a-c41ec6ffeb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 260 timestamp CSV files.\n"
     ]
    }
   ],
   "source": [
    "def save_timestamp_csv(\n",
    "    df_t,\n",
    "    *,\n",
    "    day: str,\n",
    "    time_tag: str,\n",
    "    out_base=\"Pict\",\n",
    "    suffix=\"processed\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Save processed CSV for one timestamp to:\n",
    "      Pict/dd-mm-yyyy-tttt/dd-mm-yyyy-tttt_processed.csv\n",
    "    \"\"\"\n",
    "    tttt = time_tag.replace(\".\", \"\").replace(\":\", \"\")\n",
    "    out_dir = Path(out_base) / f\"{day}-{tttt}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{day}-{tttt}_{suffix}.csv\"\n",
    "    df_t.to_csv(out_path, index=False)\n",
    "\n",
    "    return out_path\n",
    "all_saved_csv = []\n",
    "\n",
    "for day in DAYS:\n",
    "    if day not in day_dfs:\n",
    "        print(f\"[SKIP] day_dfs has no day={day}\")\n",
    "        continue\n",
    "\n",
    "    df_day = day_dfs[day]\n",
    "\n",
    "    for time_tag in sorted(df_day[\"time_tag\"].unique()):\n",
    "        df_t = df_day[df_day[\"time_tag\"] == time_tag].copy()\n",
    "\n",
    "        if df_t.empty:\n",
    "            continue\n",
    "\n",
    "        # save CSV per timestamp\n",
    "        csv_path = save_timestamp_csv(\n",
    "            df_t,\n",
    "            day=day,\n",
    "            time_tag=time_tag,\n",
    "            out_base=\"Pict\",\n",
    "        )\n",
    "        all_saved_csv.append(csv_path)\n",
    "\n",
    "print(f\"Saved {len(all_saved_csv)} timestamp CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7ffa04e-c263-4ea0-90d9-23d088e91c0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: 260\n"
     ]
    }
   ],
   "source": [
    "def plot_range2_norm_save(\n",
    "    df_t,\n",
    "    *,\n",
    "    day: str,\n",
    "    time_tag: str,\n",
    "    xcol=\"range_m\",\n",
    "    ycol=\"range2_norm\",\n",
    "    out_base=\"Pict\",\n",
    "    dpi=300,\n",
    "    figsize=(6, 4),\n",
    "    xscale=\"linear\",\n",
    "    yscale=\"linear\",\n",
    "    xlim=None,\n",
    "    ylim=(0, 1.05),\n",
    "):\n",
    "    \"\"\"\n",
    "    Save plot: range2_norm vs range_m for one timestamp\n",
    "    -> Pict/dd-mm-yyyy-tttt/range2_norm_vs_range_m.png\n",
    "    \"\"\"\n",
    "\n",
    "    # folder name\n",
    "    tttt = str(time_tag).replace(\".\", \"\").replace(\":\", \"\")  # \"00.05\" -> \"0005\"\n",
    "    out_dir = Path(out_base) / f\"{day}-{tttt}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{ycol}_vs_{xcol}.png\"\n",
    "\n",
    "    # checks\n",
    "    missing = [c for c in (xcol, ycol) if c not in df_t.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"plot_range2_norm_save: missing columns {missing}\")\n",
    "\n",
    "    # sort for clean line\n",
    "    df_t = df_t.sort_values(xcol)\n",
    "\n",
    "    # plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_t[xcol], df_t[ycol])\n",
    "\n",
    "    plt.xlabel(\"Range (m)\")\n",
    "    plt.ylabel(\"Range² normalized\")\n",
    "    plt.title(f\"{day} {time_tag}: Range² normalized vs Range\")\n",
    "\n",
    "    plt.xscale(xscale)\n",
    "    plt.yscale(yscale)\n",
    "\n",
    "    if xlim is not None:\n",
    "        plt.xlim(*xlim)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "    return out_path\n",
    "\n",
    "all_saved = []\n",
    "\n",
    "for day in DAYS:\n",
    "    if day not in day_dfs:\n",
    "        continue\n",
    "\n",
    "    df_day = day_dfs[day]\n",
    "\n",
    "    for time_tag in sorted(df_day[\"time_tag\"].unique()):\n",
    "        df_t = df_day[df_day[\"time_tag\"] == time_tag].copy()\n",
    "        if df_t.empty:\n",
    "            continue\n",
    "\n",
    "        out_path = plot_range2_norm_save(\n",
    "            df_t,\n",
    "            day=day,\n",
    "            time_tag=time_tag,\n",
    "            out_base=\"Pict\",\n",
    "            # optional:\n",
    "            # xlim=(0, 20000),\n",
    "            # yscale=\"linear\",\n",
    "        )\n",
    "        all_saved.append(out_path)\n",
    "\n",
    "print(\"Saved:\", len(all_saved))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d9b851e5-a6f4-4fe3-b511-161c9b22aab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 05-01-2026: 46 timestamps ===\n",
      "\n",
      "=== 06-01-2026: 45 timestamps ===\n",
      "\n",
      "=== 07-01-2026: 47 timestamps ===\n",
      "\n",
      "=== 08-01-2026: 46 timestamps ===\n",
      "\n",
      "=== 09-01-2026: 47 timestamps ===\n",
      "\n",
      "=== 12-01-2026: 29 timestamps ===\n",
      "\n",
      "Saved total 520 figures across 6 days.\n"
     ]
    }
   ],
   "source": [
    "def plot_one_timestamp_save(\n",
    "    df_t,\n",
    "    *,\n",
    "    day: str,\n",
    "    time_tag: str,\n",
    "    xcol=\"range_m\",\n",
    "    ycol=\"photon_counting\",\n",
    "    out_base=\"Pict\",\n",
    "    dpi=300,\n",
    "    figsize=(7, 5),\n",
    "    xscale=\"linear\",\n",
    "    yscale=\"linear\",\n",
    "):\n",
    "    tttt = str(time_tag).replace(\".\", \"\").replace(\":\", \"\")  # \"00.05\" -> \"0005\"\n",
    "    out_dir = Path(out_base) / f\"{day}-{tttt}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{ycol}_vs_{xcol}.png\"\n",
    "\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(df_t[xcol], df_t[ycol])\n",
    "\n",
    "    plt.xlabel(xcol)\n",
    "    plt.ylabel(ycol)\n",
    "    plt.title(f\"{day} {time_tag}: {ycol} vs {xcol}\")\n",
    "    plt.xscale(xscale)\n",
    "    plt.yscale(yscale)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "    return out_path\n",
    "\n",
    "\n",
    "all_saved = []\n",
    "\n",
    "#loop through each day\n",
    "for day in DAYS:\n",
    "    if day not in day_dfs:\n",
    "        print(f\"[SKIP] day_dfs has no day={day}\")\n",
    "        continue\n",
    "\n",
    "    # ensure range_m etc exist\n",
    "    day_dfs[day] = add_bin_and_snr_day(\n",
    "        day_dfs[day],\n",
    "        bin_spacing_m=bin_spacing_m,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    df_day = day_dfs[day]\n",
    "    time_tags = sorted(df_day[\"time_tag\"].unique())\n",
    "\n",
    "    print(f\"\\n=== {day}: {len(time_tags)} timestamps ===\")\n",
    "\n",
    "    for time_tag in time_tags:\n",
    "        df_t = df_day[df_day[\"time_tag\"] == time_tag].copy()\n",
    "\n",
    "        if df_t.empty:\n",
    "            print(f\"[SKIP] {day} {time_tag} empty\")\n",
    "            continue\n",
    "\n",
    "        # optional: sort for nicer line plot\n",
    "        if \"range_m\" in df_t.columns:\n",
    "            df_t = df_t.sort_values(\"range_m\")\n",
    "\n",
    "        out_path1 = plot_one_timestamp_save(\n",
    "            df_t,\n",
    "            day=day,\n",
    "            time_tag=time_tag,\n",
    "            xcol=\"range_m\",\n",
    "            ycol=\"photon_counting\",\n",
    "            out_base=\"Pict\",\n",
    "        )\n",
    "        out_path2 = plot_one_timestamp_save(\n",
    "            df_t,\n",
    "            day=day,\n",
    "            time_tag=time_tag,\n",
    "            xcol=\"range_m\",\n",
    "            ycol=\"analog\",\n",
    "            out_base=\"Pict\",\n",
    "        )\n",
    "        \n",
    "        all_saved.extend([out_path1, out_path2])\n",
    "\n",
    "\n",
    "print(f\"\\nSaved total {len(all_saved)} figures across {len(DAYS)} days.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc137b80-ad23-4200-9a6f-08525ba58763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 05-01-2026: 46 timestamps ===\n",
      "\n",
      "=== 06-01-2026: 45 timestamps ===\n",
      "\n",
      "=== 07-01-2026: 47 timestamps ===\n",
      "\n",
      "=== 08-01-2026: 46 timestamps ===\n",
      "\n",
      "=== 09-01-2026: 47 timestamps ===\n",
      "\n",
      "=== 12-01-2026: 29 timestamps ===\n",
      "\n",
      "Saved total 260 twin-axis figures across 6 days.\n"
     ]
    }
   ],
   "source": [
    "def plot_one_timestamp_twin_save(\n",
    "    df_t,\n",
    "    *,\n",
    "    day: str,\n",
    "    time_tag: str,\n",
    "    xcol=\"range_m\",\n",
    "    y1col=\"photon_counting\",\n",
    "    y2col=\"analog\",\n",
    "    out_base=\"Pict\",\n",
    "    dpi=300,\n",
    "    figsize=(5, 5),\n",
    "    xlim=None,\n",
    "    y1lim=None,\n",
    "    y2lim=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Plot twin-y figure:\n",
    "      Left axis  -> y1col (photon)\n",
    "      Right axis -> y2col (analog)\n",
    "\n",
    "    Saved to: Pict/dd-mm-yyyy-tttt/\n",
    "    \"\"\"\n",
    "\n",
    "    # folder name\n",
    "    tttt = str(time_tag).replace(\".\", \"\").replace(\":\", \"\")\n",
    "    out_dir = Path(out_base) / f\"{day}-{tttt}\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    out_path = out_dir / f\"{y1col}_and_{y2col}_vs_{xcol}.png\"\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=figsize)\n",
    "\n",
    "    # ---- Left axis (Photon) ----\n",
    "    ax1.plot(df_t[xcol], df_t[y1col], color=\"tab:blue\", label=y1col)\n",
    "    ax1.set_xlabel(\"Range (m)\", fontsize=14)\n",
    "    ax1.set_ylabel(\"Photon rate\", color=\"tab:blue\", fontsize=14)\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=\"tab:blue\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    if xlim is not None:\n",
    "        ax1.set_xlim(*xlim)\n",
    "    if y1lim is not None:\n",
    "        ax1.set_ylim(*y1lim)\n",
    "\n",
    "    # ---- Right axis (Analog) ----\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(df_t[xcol], df_t[y2col], color=\"tab:red\", label=y2col)\n",
    "    ax2.set_ylabel(\"Analog signal\", color=\"tab:red\", fontsize=14)\n",
    "    ax2.tick_params(axis=\"y\", labelcolor=\"tab:red\")\n",
    "\n",
    "    if y2lim is not None:\n",
    "        ax2.set_ylim(*y2lim)\n",
    "\n",
    "    # ---- Title & save ----\n",
    "    plt.title(f\"{day} {time_tag}: Photon & Analog vs Range\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(out_path, dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "    return out_path\n",
    "    \n",
    "all_saved = []\n",
    "\n",
    "for day in DAYS:\n",
    "    if day not in day_dfs:\n",
    "        print(f\"[SKIP] day_dfs has no day={day}\")\n",
    "        continue\n",
    "\n",
    "    # ensure range_m, bin_index, SNR exist\n",
    "    day_dfs[day] = add_bin_and_snr_day(\n",
    "        day_dfs[day],\n",
    "        bin_spacing_m=bin_spacing_m,\n",
    "        config=config\n",
    "    )\n",
    "\n",
    "    df_day = day_dfs[day]\n",
    "    time_tags = sorted(df_day[\"time_tag\"].unique())\n",
    "\n",
    "    print(f\"\\n=== {day}: {len(time_tags)} timestamps ===\")\n",
    "\n",
    "    for time_tag in time_tags:\n",
    "        df_t = df_day[df_day[\"time_tag\"] == time_tag].copy()\n",
    "\n",
    "        if df_t.empty:\n",
    "            print(f\"[SKIP] {day} {time_tag} empty\")\n",
    "            continue\n",
    "\n",
    "        # sort by range for clean plot\n",
    "        df_t = df_t.sort_values(\"range_m\")\n",
    "\n",
    "        # ---- twin-axis plot ----\n",
    "        out_path = plot_one_timestamp_twin_save(\n",
    "            df_t,\n",
    "            day=day,\n",
    "            time_tag=time_tag,\n",
    "            xcol=\"range_m\",\n",
    "            y1col=\"photon_counting\",\n",
    "            y2col=\"analog\",\n",
    "            out_base=\"Pict\",\n",
    "            # optional limits:\n",
    "            # xlim=(0, 15000),\n",
    "            # y1lim=(0, 5e6),\n",
    "            # y2lim=(0, 4096),\n",
    "        )\n",
    "\n",
    "        all_saved.append(out_path)\n",
    "\n",
    "print(f\"\\nSaved total {len(all_saved)} twin-axis figures across {len(DAYS)} days.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
